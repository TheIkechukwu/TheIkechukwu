{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8097265,"sourceType":"datasetVersion","datasetId":4781139}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/theikechukwu/price-prediction?scriptVersionId=193296296\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Linear Regression\nLinear Regression models are typically used to study the relationship between a single dependent variable $Y$ and one or more independent variable $X$. \n\nThey have an easy-to-interpret mathematical formula that can generate predictions. A simple linear regression model can be used when working with one independent variable, and a multiple regression model can be used when there are more than one independent variables.\n\nWe can start with a hypothesis that resembles the line, $Y=\\theta_0X+\\theta_1$, where $\\theta_0$ and $\\theta_1$ are the regression coefficients.\n\nNow how do we pick the values of ($\\theta_0$) and ($\\theta_1$) so that our model predictions are accurate?\n\nWe use an optimization method to minimize the loss function so as to reduce the error between model predictions and the ground truth. We start by picking random values of ($\\theta_0$) and ($\\theta_1$), and continue to update values of the coefficients till convergence. If our loss function stops decreasing, we have reached our local minima.\n\nIn multiple linear regression, we use more than one independent features ($X$) and a single dependent feature ($Y$). If we have $n$ features, our formula is as follows. Instead of considering a vector of ($m$) data entries, we will consider the ($n X m$) matrix of $X$.\n\n$Y=\\theta_0+\\theta_1X_1+\\theta_2X_2+\\theta_3X_3+...++\\theta_nX_n$\n\n## Diamond price prediction \n\nBuild a linear model that predicts the prices of diamonds based on their attributes\n\n## About the data\n\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization. However, I intend to build a model that predicts the price of the diamonds based on their attributes ","metadata":{}},{"cell_type":"code","source":"#load the necessary libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T10:31:48.882837Z","iopub.execute_input":"2024-08-20T10:31:48.883291Z","iopub.status.idle":"2024-08-20T10:32:08.626361Z","shell.execute_reply.started":"2024-08-20T10:31:48.883258Z","shell.execute_reply":"2024-08-20T10:32:08.625053Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load the data\n\ndf = pd.read_csv('/kaggle/input/diamond-dataset/diamonds.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:35:49.242773Z","iopub.execute_input":"2024-08-20T10:35:49.243586Z","iopub.status.idle":"2024-08-20T10:35:49.429911Z","shell.execute_reply.started":"2024-08-20T10:35:49.24355Z","shell.execute_reply":"2024-08-20T10:35:49.428729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Cleaning and validation \n* Check for missing values\n* Reaname columns into more meaningful names\n* Drop redundant or unnecessary columns ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:35:54.102282Z","iopub.execute_input":"2024-08-20T10:35:54.102749Z","iopub.status.idle":"2024-08-20T10:35:54.148198Z","shell.execute_reply.started":"2024-08-20T10:35:54.102714Z","shell.execute_reply":"2024-08-20T10:35:54.146919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check for missing values \ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:35:55.251866Z","iopub.execute_input":"2024-08-20T10:35:55.252288Z","iopub.status.idle":"2024-08-20T10:35:55.281909Z","shell.execute_reply.started":"2024-08-20T10:35:55.252253Z","shell.execute_reply":"2024-08-20T10:35:55.280671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset have 53,940 observations and 11 features, with no missing values","metadata":{}},{"cell_type":"code","source":"#drop duplicate values\ndf.drop_duplicates(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:35:56.82233Z","iopub.execute_input":"2024-08-20T10:35:56.822794Z","iopub.status.idle":"2024-08-20T10:35:56.85919Z","shell.execute_reply.started":"2024-08-20T10:35:56.822759Z","shell.execute_reply":"2024-08-20T10:35:56.857888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#drop the unnamed column\ndf.drop('Unnamed: 0', axis = 1, inplace = True)\n\n#rename columns into a more meaningful names\ncolnames = {'x': 'length', \n           'y': 'width',\n           'z': 'depth',\n           'depth': 'total_depth'}\ndf.rename(columns = colnames, inplace = True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:35:58.224716Z","iopub.execute_input":"2024-08-20T10:35:58.225629Z","iopub.status.idle":"2024-08-20T10:35:58.25907Z","shell.execute_reply.started":"2024-08-20T10:35:58.22558Z","shell.execute_reply":"2024-08-20T10:35:58.258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory analysis","metadata":{}},{"cell_type":"markdown","source":"Description of the columns:\n* price: price in US dollars (\\$326--\\$18,823)\n\n* carat: weight of the diamond (0.2--5.01)\n\n* cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\n* color: diamond colour, from J (worst) to D (best)\n\n* clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n* x: length in mm (0--10.74)\n\n* y: width in mm (0--58.9)\n\n* z: depth in mm (0--31.8)\n\n* depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n\n* table: width of top of diamond relative to widest point (43--95)","metadata":{}},{"cell_type":"code","source":"# Summary statistic of the data\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:00.643889Z","iopub.execute_input":"2024-08-20T10:36:00.644321Z","iopub.status.idle":"2024-08-20T10:36:00.698111Z","shell.execute_reply.started":"2024-08-20T10:36:00.644285Z","shell.execute_reply":"2024-08-20T10:36:00.696917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(pd.DataFrame(df['cut'].value_counts()))\n\nplt.figure(figsize = (10, 8))\nsns.countplot(x = df['cut'])\nplt.title('Number of diamonds based on the cut')\nplt.xlabel('Cut')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:01.922449Z","iopub.execute_input":"2024-08-20T10:36:01.922888Z","iopub.status.idle":"2024-08-20T10:36:02.328092Z","shell.execute_reply.started":"2024-08-20T10:36:01.92284Z","shell.execute_reply":"2024-08-20T10:36:02.326845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the table and visualization above, the dataset have more diamonds of Ideal cut, with 21,551 observations being ideal, followed by Premium and Very good, with Fair cut being the category with the least amount of diamonds.","metadata":{}},{"cell_type":"code","source":"display(pd.DataFrame(df.groupby('cut')['price'].mean()))\n\n# The average price of diamonds based on their cut\nplt.figure(figsize = (10, 8))\nsns.barplot(x = df['cut'], y = df['price'])\nplt.title('The average price of diamonds based on their cut')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:03.985987Z","iopub.execute_input":"2024-08-20T10:36:03.986449Z","iopub.status.idle":"2024-08-20T10:36:04.782075Z","shell.execute_reply.started":"2024-08-20T10:36:03.986415Z","shell.execute_reply":"2024-08-20T10:36:04.780796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is significant difference in the average price of diamonds based on their cut, with ideal having the least average price which is counter intuitive as you would normally expect diamonds with an ideal cut to cost averagely more than diamonds with fair cut.\n\nTo confirm there's a statiscally significant difference between the average prices of the categories as the graph suggested, I performed an Anova test on the groups ","metadata":{}},{"cell_type":"code","source":"# To confirm if there's significant difference in the mean price of diamonds based on cut\n\nideal_cut = np.array(df[df['cut'] == 'Ideal']['price'])\npremium_cut = np.array(df[df['cut'] == 'Premium']['price'])\ngood_cut = np.array(df[df['cut'] == 'Good']['price'])\nvery_good_cut = np.array(df[df['cut'] == 'Very Good']['price'])\nfair_cut = np.array(df[df['cut'] == 'Fair']['price'])\n\n# Perform one-way ANOVA\nf_statistic, p_value = stats.f_oneway(ideal_cut, premium_cut, good_cut, very_good_cut, fair_cut)\n\n# Print the results\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\n# With p-value of 8.43e-150, we can reject the null hypothesis and infer there's significant difference in the \n# mean price of diamonds across the cuts ","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:10.943456Z","iopub.execute_input":"2024-08-20T10:36:10.943906Z","iopub.status.idle":"2024-08-20T10:36:11.019001Z","shell.execute_reply.started":"2024-08-20T10:36:10.943843Z","shell.execute_reply":"2024-08-20T10:36:11.01779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With p-value of 8.43e-150, we can reject the null hypothesis and infer there's significant difference in the mean price of diamonds across the cuts.","metadata":{}},{"cell_type":"code","source":"# count of daimonds based on their clarity\ndisplay(pd.DataFrame(df['clarity'].value_counts()))\n\nplt.figure(figsize = (10, 8))\nsns.countplot(x = df['clarity'])\nplt.title('Number of diamonds based on clarity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:13.133329Z","iopub.execute_input":"2024-08-20T10:36:13.133753Z","iopub.status.idle":"2024-08-20T10:36:13.464027Z","shell.execute_reply.started":"2024-08-20T10:36:13.133723Z","shell.execute_reply":"2024-08-20T10:36:13.462802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.barplot(x = df['clarity'], y = df['price'])\nplt.title('The average price of diamonds based on their clarity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:17.765089Z","iopub.execute_input":"2024-08-20T10:36:17.765488Z","iopub.status.idle":"2024-08-20T10:36:18.730346Z","shell.execute_reply.started":"2024-08-20T10:36:17.765457Z","shell.execute_reply":"2024-08-20T10:36:18.728907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The visualizations above shows that category SI1 have the highest number of diamonds in the dataset, and on average category SI2 have the most expensive diamonds.","metadata":{}},{"cell_type":"code","source":"display(pd.DataFrame(df['clarity'].value_counts()))\n\nplt.figure(figsize = (10, 8))\nsns.countplot(x = df['color'])\nplt.title('Number of diamonds based on color')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:25.427917Z","iopub.execute_input":"2024-08-20T10:36:25.428627Z","iopub.status.idle":"2024-08-20T10:36:25.799524Z","shell.execute_reply.started":"2024-08-20T10:36:25.428577Z","shell.execute_reply":"2024-08-20T10:36:25.798364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The diamonds' colors are ranked from D (best) to J (Worst). The dataset have the highest number of diamonds that are ranked G. ","metadata":{}},{"cell_type":"code","source":"display(pd.DataFrame(df.groupby(['color', 'cut'])['price'].count()))\n\nplt.figure(figsize = (10, 8))\nsns.countplot(x = df['color'], hue = df['cut'])\nplt.title('Number of diamonds based on color and cut')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:30.203322Z","iopub.execute_input":"2024-08-20T10:36:30.203727Z","iopub.status.idle":"2024-08-20T10:36:30.807254Z","shell.execute_reply.started":"2024-08-20T10:36:30.203695Z","shell.execute_reply":"2024-08-20T10:36:30.805724Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the visualization above, there seems to be a relationship between color and cut variable which might've played a role in diamonds with fair cut having the highest average price as I shown earlier. This is because diamonds with fair cut are higher in higher rank compared to lower rank.","metadata":{}},{"cell_type":"code","source":"display(pd.DataFrame(df.groupby(['color', 'cut'])['price'].mean()))\n\nplt.figure(figsize = (10, 8))\nsns.barplot(x = df['color'], y = df['price'])\nplt.title('The average price of diamonds based on their color')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:33.322587Z","iopub.execute_input":"2024-08-20T10:36:33.323063Z","iopub.status.idle":"2024-08-20T10:36:34.273353Z","shell.execute_reply.started":"2024-08-20T10:36:33.323027Z","shell.execute_reply":"2024-08-20T10:36:34.272026Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"On average, J costs more than the rest which is also unexpected. However, a closer look look at the table further suggests there is a relationship between the two variables, cut and color.\n\nTo test if there's an interaction effect between the two variables I conducted a Two-Anova test","metadata":{}},{"cell_type":"code","source":"model = ols('price ~ color + cut + color:cut', data = df).fit()\nanova_table = sm.stats.anova_lm(model, typ = 2)\nprint(anova_table)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:39.28618Z","iopub.execute_input":"2024-08-20T10:36:39.28658Z","iopub.status.idle":"2024-08-20T10:36:40.114799Z","shell.execute_reply.started":"2024-08-20T10:36:39.286541Z","shell.execute_reply":"2024-08-20T10:36:40.113413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the Anova table, a 5% significance value, there is a statistically significant interaction between the two variables; cut and color","metadata":{}},{"cell_type":"code","source":"# Visualizing the distribution of the continuous features \ncontinuous_features = ['carat', 'depth', 'table', 'price', 'length', 'width', 'depth']\n\nfig, axes = plt.subplots(nrows = 1, ncols = len(continuous_features), figsize =(15, 5))\n\nfor i, var in enumerate(continuous_features):\n    sns.histplot(df[var],color = 'blue', kde = True, ax = axes[i])\n    axes[i].set_title(f'Distribution of {var}')\n    axes[i].set_xlabel(var)\n    axes[i].set_ylabel('Frequency')\n\n#adjust layout \nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:41.782521Z","iopub.execute_input":"2024-08-20T10:36:41.78295Z","iopub.status.idle":"2024-08-20T10:36:49.909494Z","shell.execute_reply.started":"2024-08-20T10:36:41.782917Z","shell.execute_reply":"2024-08-20T10:36:49.907871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The distribution of the continuous variables shows that most of the variables are skewed to the right apart from the variable 'length'","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection\n\nTo select the important features for our model, we need to have find out the features that are strongly correlated to the target variable, price.","metadata":{}},{"cell_type":"code","source":"df['area'] = df['length'] * df['width']\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:54.902512Z","iopub.execute_input":"2024-08-20T10:36:54.902955Z","iopub.status.idle":"2024-08-20T10:36:54.932541Z","shell.execute_reply.started":"2024-08-20T10:36:54.90292Z","shell.execute_reply":"2024-08-20T10:36:54.93143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:56.490496Z","iopub.execute_input":"2024-08-20T10:36:56.490968Z","iopub.status.idle":"2024-08-20T10:37:28.627172Z","shell.execute_reply.started":"2024-08-20T10:36:56.490932Z","shell.execute_reply":"2024-08-20T10:37:28.626085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the correlation between the features \ncorrelation_matrix = df[['carat', 'total_depth', 'table', 'price', 'length', 'width', 'depth', 'area']].corr()\ncorrelation_matrix","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:06.575785Z","iopub.execute_input":"2024-08-20T10:40:06.576974Z","iopub.status.idle":"2024-08-20T10:40:06.613436Z","shell.execute_reply.started":"2024-08-20T10:40:06.576928Z","shell.execute_reply":"2024-08-20T10:40:06.61211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.heatmap(correlation_matrix, annot = True, cmap = 'coolwarm', fmt = '.2f', linewidth = 0.5)\nplt.title('Correlation heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:10.35934Z","iopub.execute_input":"2024-08-20T10:40:10.359798Z","iopub.status.idle":"2024-08-20T10:40:10.894976Z","shell.execute_reply.started":"2024-08-20T10:40:10.359764Z","shell.execute_reply":"2024-08-20T10:40:10.893758Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the heatmap and correlation matrix above; carat, length, width, area, and depth are strongly correlated with price i.e they have a linear relationship. \n\n## The Prediction Model\n\nSince the target variable is a continuous variable, linear regression is the easy option ","metadata":{}},{"cell_type":"code","source":"Y = df['price'].to_numpy(dtype = float)\nY","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:15.749766Z","iopub.execute_input":"2024-08-20T10:40:15.750273Z","iopub.status.idle":"2024-08-20T10:40:15.759347Z","shell.execute_reply.started":"2024-08-20T10:40:15.750236Z","shell.execute_reply":"2024-08-20T10:40:15.757978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df[['carat','depth', 'color', 'cut', 'clarity', 'area', 'length', 'width']]\nX = pd.get_dummies(X).to_numpy(dtype = float)\nX","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:19.137352Z","iopub.execute_input":"2024-08-20T10:40:19.137748Z","iopub.status.idle":"2024-08-20T10:40:19.180779Z","shell.execute_reply.started":"2024-08-20T10:40:19.137716Z","shell.execute_reply":"2024-08-20T10:40:19.179529Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:21.017451Z","iopub.execute_input":"2024-08-20T10:40:21.017878Z","iopub.status.idle":"2024-08-20T10:40:21.075306Z","shell.execute_reply.started":"2024-08-20T10:40:21.017829Z","shell.execute_reply":"2024-08-20T10:40:21.074146Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#instantiate the model\nlr = LinearRegression()\n\nlr = lr.fit(X_train, Y_train)\nprint('Trainning accuracy:', r2_score(Y_train, lr.predict(X_train)))","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:24.369781Z","iopub.execute_input":"2024-08-20T10:40:24.370241Z","iopub.status.idle":"2024-08-20T10:40:24.441111Z","shell.execute_reply.started":"2024-08-20T10:40:24.370205Z","shell.execute_reply":"2024-08-20T10:40:24.439556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = lr.predict(X_test)\nprint('Coefficient of determination :', r2_score(Y_test, y_pred))\nprint('Root mean squared error:', np.sqrt(mean_squared_error(Y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:50.679572Z","iopub.execute_input":"2024-08-20T10:40:50.680021Z","iopub.status.idle":"2024-08-20T10:40:50.691484Z","shell.execute_reply.started":"2024-08-20T10:40:50.679987Z","shell.execute_reply":"2024-08-20T10:40:50.689796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model was able to explain 92% of variabilty in the target variable. The model did not just perform well on the train dataset but it also performed well on test data.\n\n## Ridge Regression\n\nTo see if building a more complex model can model the relationship between the variables better, I built a Ridge regression model.","metadata":{}},{"cell_type":"code","source":"ridge = Ridge()\nparameters = {'alpha': [0.01, 0.1, 1.0, 10.0]}\ngrid_search = GridSearchCV(estimator = ridge, param_grid = parameters, cv = 5)\ngrid_search.fit(X_train, Y_train)\nprint('The best Alpha value:', grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:54.14063Z","iopub.execute_input":"2024-08-20T10:40:54.141072Z","iopub.status.idle":"2024-08-20T10:40:54.650871Z","shell.execute_reply.started":"2024-08-20T10:40:54.141038Z","shell.execute_reply":"2024-08-20T10:40:54.649271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ridge = Ridge(alpha = 10)\nridge.fit(X_train, Y_train)\nprint('Coefficient of determination :', r2_score(Y_test, ridge.predict(X_test)))\nprint('Root mean squared error:', np.sqrt(mean_squared_error(Y_test, ridge.predict(X_test))))","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:40:56.167741Z","iopub.execute_input":"2024-08-20T10:40:56.168179Z","iopub.status.idle":"2024-08-20T10:40:56.194263Z","shell.execute_reply.started":"2024-08-20T10:40:56.168138Z","shell.execute_reply":"2024-08-20T10:40:56.192821Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Even after regularization, there was no significant improvement in the performance of the model. ","metadata":{}},{"cell_type":"markdown","source":"## Neural Network Model","metadata":{}},{"cell_type":"code","source":"output_size=1\nhidden_layer=3\ninput_size=1\nlearning_rate=0.01\nloss_function='mean_squared_error'\nepochs=50\nbatch_size=10","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:41:00.721641Z","iopub.execute_input":"2024-08-20T10:41:00.722153Z","iopub.status.idle":"2024-08-20T10:41:00.729048Z","shell.execute_reply.started":"2024-08-20T10:41:00.722114Z","shell.execute_reply":"2024-08-20T10:41:00.727565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(keras.layers.Dense(hidden_layer, activation = 'relu'))\nmodel.add(keras.layers.Dense(output_size))\nmodel.compile(keras.optimizers.Adam(learning_rate = learning_rate), loss_function)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:41:02.940708Z","iopub.execute_input":"2024-08-20T10:41:02.942219Z","iopub.status.idle":"2024-08-20T10:41:03.005323Z","shell.execute_reply.started":"2024-08-20T10:41:02.942162Z","shell.execute_reply":"2024-08-20T10:41:03.003915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size,\n                    verbose = False, validation_split = 0.3)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:41:07.296251Z","iopub.execute_input":"2024-08-20T10:41:07.296694Z","iopub.status.idle":"2024-08-20T10:44:39.507199Z","shell.execute_reply.started":"2024-08-20T10:41:07.296661Z","shell.execute_reply":"2024-08-20T10:44:39.505956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.legend()\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:47:02.805007Z","iopub.execute_input":"2024-08-20T10:47:02.805961Z","iopub.status.idle":"2024-08-20T10:47:02.812242Z","shell.execute_reply.started":"2024-08-20T10:47:02.805919Z","shell.execute_reply":"2024-08-20T10:47:02.810991Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss(history)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:47:04.675458Z","iopub.execute_input":"2024-08-20T10:47:04.675873Z","iopub.status.idle":"2024-08-20T10:47:05.028835Z","shell.execute_reply.started":"2024-08-20T10:47:04.675826Z","shell.execute_reply":"2024-08-20T10:47:05.027601Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint('Coefficient of determination: ', r2_score(Y_test, y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(Y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:47:09.924628Z","iopub.execute_input":"2024-08-20T10:47:09.925067Z","iopub.status.idle":"2024-08-20T10:47:10.828218Z","shell.execute_reply.started":"2024-08-20T10:47:09.925033Z","shell.execute_reply":"2024-08-20T10:47:10.826893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The NN model performed significantly better than both Linear regression model and Ridge regression, with RMSE of 685 and Coefficient of determination value of 0.97 meaning the NN model was able to explain 97% of the variability in the test data. ","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(keras.layers.Dense(10, activation = 'relu'))\nmodel.add(keras.layers.Dense(output_size))\nmodel.compile(keras.optimizers.Adam(learning_rate = learning_rate), loss_function)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T11:14:35.320141Z","iopub.execute_input":"2024-08-20T11:14:35.321064Z","iopub.status.idle":"2024-08-20T11:14:35.336091Z","shell.execute_reply.started":"2024-08-20T11:14:35.321023Z","shell.execute_reply":"2024-08-20T11:14:35.334828Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size,\n                    verbose = False, validation_split = 0.3)","metadata":{"execution":{"iopub.status.idle":"2024-08-20T11:19:26.427745Z","shell.execute_reply.started":"2024-08-20T11:15:55.440419Z","shell.execute_reply":"2024-08-20T11:19:26.426748Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss(history)\ny_pred = model.predict(X_test)\nprint('Coefficient of determination: ', r2_score(Y_test, y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(Y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2024-08-20T11:19:36.393915Z","iopub.execute_input":"2024-08-20T11:19:36.394303Z","iopub.status.idle":"2024-08-20T11:19:37.557002Z","shell.execute_reply.started":"2024-08-20T11:19:36.394273Z","shell.execute_reply":"2024-08-20T11:19:37.555802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Changing the number of hidden layers to 10 slightly improved the model performance of the model ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}